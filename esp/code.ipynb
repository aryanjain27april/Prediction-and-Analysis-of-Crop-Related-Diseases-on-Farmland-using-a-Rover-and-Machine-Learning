{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v6.1-223-g1dcb774 Python-3.8.5 torch-1.11.0 CUDA:0 (GeForce MX230, 2048MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model summary: 213 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import cvlib as cv\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from cvlib.object_detection import draw_bbox\n",
    "import concurrent.futures\n",
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "url='http://192.168.1.10/cam-mid.jpg'\n",
    "im=None\n",
    "\n",
    "print(f\"[INFO] Loading model... \")\n",
    "## loading the custom trained model\n",
    "# model =  torch.hub.load('ultralytics/yolov5', 'custom', path='last.pt',force_reload=True) ## if you want to download the git repo and then run the detection\n",
    "model =  torch.hub.load('D:\\8th_Sem_Proj\\Dataset\\8th_Sem_Proj\\With_YoloV5\\yolov5', 'custom', source ='local', path=r'D:\\8th_Sem_Proj\\Dataset\\8th_Sem_Proj\\With_YoloV5\\yolov5\\runs\\train\\exp9\\weights\\best.pt',force_reload=True) ### The repo is stored locally\n",
    "x = None\n",
    "classes = model.names ### class names in string format\n",
    "coordinates = (100,100)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "fontScale = 1\n",
    "color = (255,0,255)\n",
    "thickness = 2\n",
    "def gen_frames():\n",
    "    x = None\n",
    "    #cv2.namedWindow(\"live transmission\", cv2.WINDOW_AUTOSIZE)\n",
    "    while True:\n",
    "        img_resp=urllib.request.urlopen(url)\n",
    "        #x = cv2.imread(bytearray(img_resp.read()),dtype=np.uint8)\n",
    "        imgnp=np.array(bytearray(img_resp.read()),dtype=np.uint8)\n",
    "        im = cv2.imdecode(imgnp,-1)\n",
    "        results = d(im)\n",
    "        results = results.xyxyn[0][:, -1]\n",
    "        #print(im)\n",
    "        if len(results)!=0:\n",
    "            if results == 0:\n",
    "                text = 'Early_Blight'\n",
    "                x=0\n",
    "            elif results == 1:\n",
    "                text = 'Healthy'\n",
    "                x=1\n",
    "            elif results == 2:\n",
    "                text = 'Mold_leaf'\n",
    "                x=2\n",
    "        if x:\n",
    "            im = cv2.putText(im, text, coordinates, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        yield (b'--frame\\r\\n'\n",
    "                   b'Content-Type: image/jpeg\\r\\n\\r\\n' + im + b'\\r\\n')\n",
    "        #cv2.imshow('live transmission',im)\n",
    "        \n",
    "        # key=cv2.waitKey(5)\n",
    "        # if key==ord('q'):\n",
    "        #     break\n",
    "        #break\n",
    "    #return im\n",
    "    #cv2.destroyAllWindows()\n",
    "\n",
    "def run2():\n",
    "    cv2.namedWindow(\"detection\", cv2.WINDOW_AUTOSIZE)\n",
    "    while True:\n",
    "        img_resp=urllib.request.urlopen(url)\n",
    "        imgnp=np.array(bytearray(img_resp.read()),dtype=np.uint8)\n",
    "        im = cv2.imdecode(imgnp,-1)\n",
    "        #cv.imshow(im)\n",
    "        bbox, label, conf = cv.detect_common_objects(im)\n",
    "        # print(bbox)\n",
    "        # print(label)\n",
    "        # print(conf)\n",
    "        im = draw_bbox(im, bbox, label, conf)\n",
    "\n",
    "        cv2.imshow('detection',im)\n",
    "        key=cv2.waitKey(5)\n",
    "        if key==ord('q'):\n",
    "            break\n",
    "        #break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model... \n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\8th_Sem_Proj\\Dataset\\8th_Sem_Proj\\esp\\code.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/esp/code.ipynb#ch0000008?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[INFO] Loading model... \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/esp/code.ipynb#ch0000008?line=14'>15</a>\u001b[0m \u001b[39m## loading the custom trained model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/esp/code.ipynb#ch0000008?line=15'>16</a>\u001b[0m \u001b[39m# model =  torch.hub.load('ultralytics/yolov5', 'custom', path='last.pt',force_reload=True) ## if you want to download the git repo and then run the detection\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/esp/code.ipynb#ch0000008?line=16'>17</a>\u001b[0m model \u001b[39m=\u001b[39m  torch\u001b[39m.\u001b[39;49mhub\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mD:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m8th_Sem_Proj\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDataset\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m8th_Sem_Proj\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mWith_YoloV5\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39myolov5\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcustom\u001b[39;49m\u001b[39m'\u001b[39;49m, source \u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlocal\u001b[39;49m\u001b[39m'\u001b[39;49m, path\u001b[39m=\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mD:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m8th_Sem_Proj\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDataset\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m8th_Sem_Proj\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mWith_YoloV5\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39myolov5\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mruns\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mexp9\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mweights\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mbest.pt\u001b[39;49m\u001b[39m'\u001b[39;49m,force_reload\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \u001b[39m### The repo is stored locally\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/esp/code.ipynb#ch0000008?line=17'>18</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/esp/code.ipynb#ch0000008?line=18'>19</a>\u001b[0m classes \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mnames \u001b[39m### class names in string format\u001b[39;00m\n",
      "File \u001b[1;32md:\\Conda\\envs\\rover_yolov5_1\\lib\\site-packages\\torch\\hub.py:404\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Conda/envs/rover_yolov5_1/lib/site-packages/torch/hub.py?line=400'>401</a>\u001b[0m \u001b[39mif\u001b[39;00m source \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgithub\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='file:///d%3A/Conda/envs/rover_yolov5_1/lib/site-packages/torch/hub.py?line=401'>402</a>\u001b[0m     repo_or_dir \u001b[39m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, verbose, skip_validation)\n\u001b[1;32m--> <a href='file:///d%3A/Conda/envs/rover_yolov5_1/lib/site-packages/torch/hub.py?line=403'>404</a>\u001b[0m model \u001b[39m=\u001b[39m _load_local(repo_or_dir, model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///d%3A/Conda/envs/rover_yolov5_1/lib/site-packages/torch/hub.py?line=404'>405</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32md:\\Conda\\envs\\rover_yolov5_1\\lib\\site-packages\\torch\\hub.py:433\u001b[0m, in \u001b[0;36m_load_local\u001b[1;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Conda/envs/rover_yolov5_1/lib/site-packages/torch/hub.py?line=429'>430</a>\u001b[0m hub_module \u001b[39m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[0;32m    <a href='file:///d%3A/Conda/envs/rover_yolov5_1/lib/site-packages/torch/hub.py?line=431'>432</a>\u001b[0m entry \u001b[39m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[1;32m--> <a href='file:///d%3A/Conda/envs/rover_yolov5_1/lib/site-packages/torch/hub.py?line=432'>433</a>\u001b[0m model \u001b[39m=\u001b[39m entry(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///d%3A/Conda/envs/rover_yolov5_1/lib/site-packages/torch/hub.py?line=434'>435</a>\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mremove(hubconf_dir)\n\u001b[0;32m    <a href='file:///d%3A/Conda/envs/rover_yolov5_1/lib/site-packages/torch/hub.py?line=436'>437</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mD:\\8th_Sem_Proj\\Dataset\\8th_Sem_Proj\\With_YoloV5\\yolov5\\hubconf.py:71\u001b[0m, in \u001b[0;36mcustom\u001b[1;34m(path, autoshape, _verbose, device)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/With_YoloV5/yolov5/hubconf.py?line=68'>69</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcustom\u001b[39m(path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpath/to/model.pt\u001b[39m\u001b[39m'\u001b[39m, autoshape\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, _verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     <a href='file:///d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/With_YoloV5/yolov5/hubconf.py?line=69'>70</a>\u001b[0m     \u001b[39m# YOLOv5 custom or local model\u001b[39;00m\n\u001b[1;32m---> <a href='file:///d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/With_YoloV5/yolov5/hubconf.py?line=70'>71</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _create(path, autoshape\u001b[39m=\u001b[39;49mautoshape, verbose\u001b[39m=\u001b[39;49m_verbose, device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[1;32mD:\\8th_Sem_Proj\\Dataset\\8th_Sem_Proj\\With_YoloV5\\yolov5\\hubconf.py:31\u001b[0m, in \u001b[0;36m_create\u001b[1;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/With_YoloV5/yolov5/hubconf.py?line=14'>15</a>\u001b[0m \u001b[39m\"\"\"Creates or loads a YOLOv5 model\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/With_YoloV5/yolov5/hubconf.py?line=15'>16</a>\u001b[0m \n\u001b[0;32m     <a href='file:///d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/With_YoloV5/yolov5/hubconf.py?line=16'>17</a>\u001b[0m \u001b[39mArguments:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/With_YoloV5/yolov5/hubconf.py?line=26'>27</a>\u001b[0m \u001b[39m    YOLOv5 model\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/With_YoloV5/yolov5/hubconf.py?line=27'>28</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/With_YoloV5/yolov5/hubconf.py?line=28'>29</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[1;32m---> <a href='file:///d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/With_YoloV5/yolov5/hubconf.py?line=30'>31</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoShape, DetectMultiBackend\n\u001b[0;32m     <a href='file:///d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/With_YoloV5/yolov5/hubconf.py?line=31'>32</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39myolo\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n\u001b[0;32m     <a href='file:///d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/With_YoloV5/yolov5/hubconf.py?line=32'>33</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdownloads\u001b[39;00m \u001b[39mimport\u001b[39;00m attempt_download\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import cvlib as cv\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from cvlib.object_detection import draw_bbox\n",
    "import concurrent.futures\n",
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "url='http://192.168.1.10/cam-mid.jpg'\n",
    "im=None\n",
    "\n",
    "print(f\"[INFO] Loading model... \")\n",
    "## loading the custom trained model\n",
    "# model =  torch.hub.load('ultralytics/yolov5', 'custom', path='last.pt',force_reload=True) ## if you want to download the git repo and then run the detection\n",
    "model =  torch.hub.load('D:\\8th_Sem_Proj\\Dataset\\8th_Sem_Proj\\With_YoloV5\\yolov5', 'custom', source ='local', path=r'D:\\8th_Sem_Proj\\Dataset\\8th_Sem_Proj\\With_YoloV5\\yolov5\\runs\\train\\exp9\\weights\\best.pt',force_reload=True) ### The repo is stored locally\n",
    "x = None\n",
    "classes = model.names ### class names in string format\n",
    "coordinates = (100,100)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "fontScale = 1\n",
    "color = (255,0,255)\n",
    "thickness = 2\n",
    "def run1():\n",
    "    x = None\n",
    "    cv2.namedWindow(\"live transmission\", cv2.WINDOW_AUTOSIZE)\n",
    "    while True:\n",
    "        img_resp=urllib.request.urlopen(url)\n",
    "        #x = cv2.imread(bytearray(img_resp.read()),dtype=np.uint8)\n",
    "        imgnp=np.array(bytearray(img_resp.read()),dtype=np.uint8)\n",
    "        im = cv2.imdecode(imgnp,-1)\n",
    "        results = d(im)\n",
    "        results = results.xyxyn[0][:, -1]\n",
    "        #print(im)\n",
    "        if len(results)!=0:\n",
    "            if results == 0:\n",
    "                text = 'Early_Blight'\n",
    "                x=0\n",
    "            elif results == 1:\n",
    "                text = 'Healthy'\n",
    "                x=1\n",
    "            elif results == 2:\n",
    "                text = 'Mold_leaf'\n",
    "                x=2\n",
    "        if x:\n",
    "            im = cv2.putText(im, text, coordinates, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('live transmission',im)\n",
    "        \n",
    "        key=cv2.waitKey(5)\n",
    "        if key==ord('q'):\n",
    "            break\n",
    "        #break\n",
    "    #return im\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def run2():\n",
    "    cv2.namedWindow(\"detection\", cv2.WINDOW_AUTOSIZE)\n",
    "    while True:\n",
    "        img_resp=urllib.request.urlopen(url)\n",
    "        imgnp=np.array(bytearray(img_resp.read()),dtype=np.uint8)\n",
    "        im = cv2.imdecode(imgnp,-1)\n",
    "        #cv.imshow(im)\n",
    "        bbox, label, conf = cv.detect_common_objects(im)\n",
    "        # print(bbox)\n",
    "        # print(label)\n",
    "        # print(conf)\n",
    "        im = draw_bbox(im, bbox, label, conf)\n",
    "\n",
    "        cv2.imshow('detection',im)\n",
    "        key=cv2.waitKey(5)\n",
    "        if key==ord('q'):\n",
    "            break\n",
    "        #break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d(im):\n",
    "\n",
    "\n",
    "\n",
    "        ### -------------------------------------- function to run detection ---------------------------------------------------------\n",
    "        def detectx (frame, model):\n",
    "            frame = [frame]\n",
    "            #print(f\"[INFO] Detecting. . . \")\n",
    "            results = model(frame)\n",
    "            # results.show()\n",
    "            # print( results.xyxyn[0])\n",
    "            # print(results.xyxyn[0][:, -1])\n",
    "            # print(results.xyxyn[0][:, :-1])\n",
    "            return results\n",
    "            labels, cordinates = results.xyxyn[0][:, -1], results.xyxyn[0][:, :-1]\n",
    "\n",
    "            return labels, cordinates\n",
    "\n",
    "        ### ------------------------------------ to plot the BBox and results --------------------------------------------------------\n",
    "        def plot_boxes(results, frame,classes):\n",
    "\n",
    "            \"\"\"\n",
    "            --> This function takes results, frame and classes\n",
    "            --> results: contains labels and coordinates predicted by model on the given frame\n",
    "            --> classes: contains the strting labels\n",
    "            \"\"\"\n",
    "            labels, cord = results\n",
    "            n = len(labels)\n",
    "            x_shape, y_shape = frame.shape[1], frame.shape[0]\n",
    "            if n!=0:\n",
    "                print(f\"[INFO] Total {n} detections. . . \")\n",
    "                print(f\"[INFO] Looping through all detections. . . \")\n",
    "\n",
    "\n",
    "            ### looping through the detections\n",
    "            for i in range(n):\n",
    "                row = cord[i]\n",
    "                if row[4] >= 0.55: ### threshold value for detection. We are discarding everything below this value\n",
    "                    print(f\"[INFO] Extracting BBox coordinates. . . \")\n",
    "                    x1, y1, x2, y2 = int(row[0]*x_shape), int(row[1]*y_shape), int(row[2]*x_shape), int(row[3]*y_shape) ## BBOx coordniates\n",
    "                    text_d = classes[int(labels[i])]\n",
    "\n",
    "\n",
    "                    if text_d == 'mask':\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2) ## BBox\n",
    "                        cv2.rectangle(frame, (x1, y1-20), (x2, y1), (0, 255,0), -1) ## for text label background\n",
    "\n",
    "                        \n",
    "                        cv2.putText(frame, text_d + f\" {round(float(row[4]),2)}\", (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.7,(255,255,255), 2)\n",
    "\n",
    "                    elif text_d == 'nomask':\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0,255), 2) ## BBox\n",
    "                        cv2.rectangle(frame, (x1, y1-20), (x2, y1), (0, 0,255), -1) ## for text label background\n",
    "\n",
    "                        \n",
    "                        cv2.putText(frame, text_d + f\" {round(float(row[4]),2)}\", (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.7,(255,255,255), 2)\n",
    "                    ## print(row[4], type(row[4]),int(row[4]), len(text_d))\n",
    "\n",
    "            return frame\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### ---------------------------------------------- Main function -----------------------------------------------------\n",
    "\n",
    "        def main(img_path=None, vid_path=None,vid_out = None):\n",
    "\n",
    "\n",
    "\n",
    "            #if img_path != None:\n",
    "                #print(f\"[INFO] Working with image: {img_path}\")\n",
    "                #frame = cv2.imread(img_path)\n",
    "            frame = cv2.cvtColor(img_path,cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "            results = detectx(frame, model = model) ### DETECTION HAPPENING HERE    \n",
    "\n",
    "            #print('resulta/////////\\t',results)\n",
    "            #return results\n",
    "            #frame = cv2.cvtColor(frame,cv2.COLOR_RGB2BGR)\n",
    "            #frame = plot_boxes(results, frame,classes = classes)\n",
    "            return results\n",
    "            #     cv2.namedWindow(\"img_only\", cv2.WINDOW_NORMAL) ## creating a free windown to show the result\n",
    "\n",
    "            #     while True:\n",
    "            #         # frame = cv2.cvtColor(frame,cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            #         cv2.imshow(\"img_only\", frame)\n",
    "\n",
    "            #         if cv2.waitKey(5) & 0xFF == 27:\n",
    "            #             print(f\"[INFO] Exiting. . . \")\n",
    "            #             cv2.imwrite(\"final_output.jpg\",frame) ## if you want to save he output result.\n",
    "\n",
    "            #             break\n",
    "\n",
    "            # elif vid_path !=None:\n",
    "            #     print(f\"[INFO] Working with video: {vid_path}\")\n",
    "\n",
    "            #     ## reading the video\n",
    "            #     cap = cv2.VideoCapture(vid_path)\n",
    "\n",
    "\n",
    "            #     if vid_out: ### creating the video writer if video output path is given\n",
    "\n",
    "            #         # by default VideoCapture returns float instead of int\n",
    "            #         width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            #         height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            #         fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "            #         codec = cv2.VideoWriter_fourcc(*'mp4v') ##(*'XVID')\n",
    "            #         out = cv2.VideoWriter(vid_out, codec, fps, (width, height))\n",
    "\n",
    "            #     # assert cap.isOpened()\n",
    "            #     frame_no = 1\n",
    "\n",
    "            #     cv2.namedWindow(\"vid_out\", cv2.WINDOW_NORMAL)\n",
    "            #     while True:\n",
    "            #         # start_time = time.time()\n",
    "            #         ret, frame = cap.read()\n",
    "            #         if ret :\n",
    "            #             print(f\"[INFO] Working with frame {frame_no} \")\n",
    "\n",
    "            #             frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "            #             results = detectx(frame, model = model)\n",
    "            #             frame = cv2.cvtColor(frame,cv2.COLOR_RGB2BGR)\n",
    "            #             frame = plot_boxes(results, frame,classes = classes)\n",
    "                        \n",
    "            #             cv2.imshow(\"vid_out\", frame)\n",
    "            #             if vid_out:\n",
    "            #                 print(f\"[INFO] Saving output video. . . \")\n",
    "            #                 out.write(frame)\n",
    "\n",
    "            #             if cv2.waitKey(5) & 0xFF == 27:\n",
    "            #                 break\n",
    "            #             frame_no += 1\n",
    "                \n",
    "            #     print(f\"[INFO] Clening up. . . \")\n",
    "            #     ### releaseing the writer\n",
    "            #     out.release()\n",
    "                \n",
    "            #     ## closing all windows\n",
    "            #     cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "        ### -------------------  calling the main function-------------------------------\n",
    "\n",
    "\n",
    "        # main(vid_path=\"facemask.mp4\",vid_out=\"facemask_result.mp4\") ### for custom video\n",
    "        # main(vid_path=0,vid_out=\"webcam_facemask_result.mp4\") #### for webcam\n",
    "\n",
    "        #print('main')\n",
    "        return main(im)\n",
    "        #main(img_path=\"crowd_mask181.jpg\") ## for image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Early_Blight', 'Healthy', 'mold_leaf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: NMS time limit 0.130s exceeded\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\8th_Sem_Proj\\Dataset\\8th_Sem_Proj\\esp\\code.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/esp/code.ipynb#ch0000002?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mstarted\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/esp/code.ipynb#ch0000002?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mProcessPoolExecutor() \u001b[39mas\u001b[39;00m executer:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/esp/code.ipynb#ch0000002?line=3'>4</a>\u001b[0m         run1()\n",
      "\u001b[1;32md:\\8th_Sem_Proj\\Dataset\\8th_Sem_Proj\\esp\\code.ipynb Cell 1'\u001b[0m in \u001b[0;36mrun1\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/esp/code.ipynb#ch0000000?line=34'>35</a>\u001b[0m \u001b[39m#print(im)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/esp/code.ipynb#ch0000000?line=35'>36</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(results)\u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/esp/code.ipynb#ch0000000?line=36'>37</a>\u001b[0m     \u001b[39mif\u001b[39;00m results \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/esp/code.ipynb#ch0000000?line=37'>38</a>\u001b[0m         text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mEarly_Blight\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/8th_Sem_Proj/Dataset/8th_Sem_Proj/esp/code.ipynb#ch0000000?line=38'>39</a>\u001b[0m         x\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"started\")\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executer:\n",
    "            run1()\n",
    "            #run1()\n",
    "            #f1= executer.submit(run1)\n",
    "            #f2= executer.submit(run2)\n",
    "            #print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = results.xyxyn[0][:, -1][0]\n",
    "x == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if x==0:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.], device='cuda:0')\n",
      "tensor([[0.17168, 0.24908, 0.47312, 0.50295, 0.27142]], device='cuda:0')\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "labels, cord = results\n",
    "n = len(labels)\n",
    "print(labels)\n",
    "print(cord)\n",
    "print(n)\n",
    "#x_shape, y_shape = frame.shape[1], frame.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "from flask import Flask, render_template, Response\n",
    "import cv2\n",
    "#Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "    return Response(gen_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06be1ed3d29d3a51f63ac980a9a27f5ef5735211b339e9245d1e6a8ceaeeea07"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('rover_yolov5_1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
